{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "close-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "desirable-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.a - Read frames from video\n",
    "\n",
    "def video_to_frames(vid_path: str, start_second, end_second):\n",
    "    \"\"\"\n",
    "    Load a video and return its frames from the wanted time range.\n",
    "    :param vid_path: video file path.\n",
    "    :param start_second: time of first frame to be taken from the\n",
    "    video in seconds.\n",
    "    :param end_second: time of last frame to be taken from the\n",
    "    video in seconds.\n",
    "    :return:\n",
    "    frame_set: a 4D uint8 np array of size [num_of_frames x H x W x C]\n",
    "    containing the wanted video frames.\n",
    "    \"\"\"\n",
    "\n",
    "    #calculate desired frames \n",
    "    cap = cv2.VideoCapture(vid_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    start_frame = start_second*fps\n",
    "    end_frame = end_second*fps\n",
    "    num_of_frames = 1 + (end_frame-start_frame) \n",
    "\n",
    "    #get the desired frames\n",
    "    frames = []\n",
    "    curr_frame = 0\n",
    "    i = 0\n",
    "\n",
    "    while ( i<num_of_frames):\n",
    "        valid, img = cap.read() # read one frame from the 'capture' object; img is (H, W, C)\n",
    "        curr_fr += 1\n",
    "        if (curr_frame >= start_frame): #if started to get inside the desired section, save frame\n",
    "            frames.append(img)\n",
    "            i += 1\n",
    "    frame_set = np.array(frames) # convert to np array,dimensions (T*fps, H, W, C)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    return frame_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "jewish-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_corr(corr_obj, img):\n",
    "    \"\"\"\n",
    "    return the center coordinates of the location of 'corr_obj' in 'img'.\n",
    "    :param corr_obj: 2D numpy array of size [H_obj x W_obj]\n",
    "    containing an image of a component.\n",
    "    :param img: 2D numpy array of size [H_img x W_img]\n",
    "    where H_img >= H_obj and W_img>=W_obj,\n",
    "    containing an image with the 'corr_obj' component in it.\n",
    "    :return:\n",
    "    match_coord: the two center coordinates in 'img'\n",
    "    of the 'corr_obj' component.\n",
    "    \"\"\"\n",
    " # ====== YOUR CODE: ======\n",
    " # corr_obj correltion with itself\n",
    "    match_corr_obj = cv2.filter2D(corr_obj.astype('float32'), -1, corr_obj.astype('float32'), borderType=cv2.BORDER_CONSTANT)\n",
    "\n",
    " # maximal correlation value of the corr_obj image with itself\n",
    "    max_obj = np.max(match_corr_obj) \n",
    "    \n",
    " # corr_obj correltion with itself\n",
    "    img_corr_obj = cv2.filter2D(img.astype('float32'), -1, corr_obj.astype('float32'), borderType=cv2.BORDER_CONSTANT)   \n",
    "    match_coord = np.unravel_index((np.abs(img_corr_obj - max_obj)).argmin(), img_corr_obj.shape)    \n",
    " \n",
    "\n",
    "    return match_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "behavioral-smooth",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'curr_fr' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-10eacc079577>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 2.c - Pre-Processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo_to_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../given_data/Corsica.mp4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m260\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtop_crop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-52124e754cd3>\u001b[0m in \u001b[0;36mvideo_to_frames\u001b[1;34m(vid_path, start_second, end_second)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mnum_of_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# read one frame from the 'capture' object; img is (H, W, C)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mcurr_fr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcurr_frame\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mstart_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#if started to get inside the desired section, save frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'curr_fr' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# 2.c - Pre-Processing\n",
    "\n",
    "frames = video_to_frames(\"../given_data/Corsica.mp4\", 250, 260)\n",
    "\n",
    "top_crop = int(frames[0].shape[0] / 3)\n",
    "left_crop = 7\n",
    "right_crop = 627\n",
    "\n",
    "def crop_frame(frame, top_crop, left_crop, right_crop):\n",
    "    cropped = frame[top_crop:, left_crop:right_crop]\n",
    "    return cropped\n",
    "\n",
    "cropped_frames = []\n",
    "\n",
    "for frame in frames:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cropped_frames.append(crop_frame(frame, top_crop, left_crop, right_crop))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.d - Creating the panorama base\n",
    "\n",
    "\n",
    "cropped_frames_h, cropped_frames_w = cropped_frames[0].shape\n",
    "panorama = np.zeros((cropped_frames_h, int(cropped_frames_w * 2.5)))\n",
    "\n",
    "ref_frame = cropped_frames[int(len(cropped_frames) / 2)]\n",
    "\n",
    "ref_right_edge = int((panorama.shape[1] - ref_frame.shape[1]) / 2 + ref_frame.shape[1])\n",
    "ref_left_edge = int((panorama.shape[1] - ref_frame.shape[1]) / 2)\n",
    "panorama[:, ref_left_edge:ref_right_edge] = ref_frame\n",
    "\n",
    "plt.imshow(ref_frame, cmap='gray')\n",
    "plt.title(\"original reference frame\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(panorama, cmap='gray')\n",
    "plt.title(\"Panorama image - with the reference frame in the center\")\n",
    "plt.show()\n",
    "\n",
    "ref_index = int(len(cropped_frames) / 2)\n",
    "early_frame = cropped_frames[ref_index - 70]\n",
    "late_frame = cropped_frames[ref_index + 70]\n",
    "\n",
    "plt.imshow(early_frame, cmap='gray')\n",
    "plt.title(\"early frame\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(late_frame, cmap='gray')\n",
    "plt.title(\"late frame\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.e - Frames matching\n",
    "\n",
    "sub_early_frame = early_frame[:,:300]\n",
    "sub_late_frame = late_frame[:, 400:]\n",
    "\n",
    "\n",
    "early_match_coord = match_corr(sub_early_frame, ref_frame)\n",
    "late_match_coord = match_corr(sub_late_frame, ref_frame)\n",
    "\n",
    "plt.imshow(sub_early_frame, cmap='gray')\n",
    "plt.title(early_match_coord)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(sub_late_frame, cmap='gray')\n",
    "plt.title(late_match_coord)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.f\n",
    "\n",
    "# Calculate the bounds for the left side of the panorama\n",
    "late_overlap_right_panorama =int(ref_left_edge + late_match_coord[1] + int((sub_late_frame.shape[1])/2))\n",
    "late_overlap_left_panorama = int(ref_left_edge)\n",
    "late_overlap_left = late_frame.shape[1] - (late_overlap_right_panorama - late_overlap_left_panorama)\n",
    "late_start_panorama = late_overlap_right_panorama - late_frame.shape[1]\n",
    "\n",
    "# Calculate the bounds for the right side of the panorama\n",
    "early_overlap_right_panorama = ref_right_edge\n",
    "early_overlap_left_panorama = int(ref_left_edge + early_match_coord[1] - int((sub_early_frame.shape[1])/2))\n",
    "early_overlap_right = early_overlap_right_panorama - early_overlap_left_panorama\n",
    "early_end_panorama = early_overlap_left_panorama + early_frame.shape[1]\n",
    "\n",
    "#making the left side panorama\n",
    "panorama[:, late_overlap_left_panorama:late_overlap_right_panorama] = ((panorama[:, late_overlap_left_panorama:late_overlap_right_panorama] + late_frame[:,late_overlap_left:])/2)\n",
    "panorama[:, late_start_panorama:late_overlap_left_panorama] = late_frame[:,:late_overlap_left]\n",
    "\n",
    "#making the right side panorama\n",
    "panorama[:, early_overlap_left_panorama:early_overlap_right_panorama] = ((panorama[:, early_overlap_left_panorama:early_overlap_right_panorama] + early_frame[:, :early_overlap_right])/2)\n",
    "panorama[:, ref_right_edge:early_end_panorama] = early_frame[:,early_overlap_right:]\n",
    "\n",
    "\n",
    "plt.imshow(panorama, cmap='gray')\n",
    "plt.title(\"final panorama\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-silence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
